{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#primary requirements: # python3, numpy, pandas, xarray, dask, bottleneck, netCDF4, cftime, matplotlib\n",
    "# run this script to check if all dependencies are installed and working\n",
    "# also fixes np.NaN to np.nan in marineHeatWaves module\n",
    "\n",
    "import sys, platform\n",
    "import numpy as np\n",
    "import pandas as pd, xarray as xr, dask, bottleneck, netCDF4, cftime, matplotlib\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "from glob import glob\n",
    "\n",
    "# convert np.NAN to np.nan to avoid dependency issues\n",
    "if not hasattr(np, \"NaN\"):\n",
    "    np.NaN = np.nan  \n",
    "\n",
    "print(sys.executable)\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb328cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create daily mean files data from 3-hourly ERA5 wind data\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "# set the file paths\n",
    "# input 3-hourly files path\n",
    "SRC_GLOB = \"/home/Desktop/ERA5_wind_data_30S_30N_20E_110E/era5_wind_*.nc\"\n",
    "# output daily mean files path\n",
    "OUT_DIR  = Path(\"/home/Desktop/Jupyter files/outputs/wind/daily\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tname = \"valid_time\"     # file has 'valid_time' variable instead of 'time'\n",
    "# variable name\n",
    "u_name, v_name = \"u10\", \"v10\"\n",
    "\n",
    "# write-friendly chunks (daily ~ 365 along time)\n",
    "# lat/lon sizes below assume your full 30S–30N, 20–110E 0.25° grid (241x361). Adjust if yours differ.\n",
    "ENC = {\n",
    "    u_name: {\"zlib\": True, \"complevel\": 4, \"dtype\": \"float32\", \"chunksizes\": (90, 120, 120)},\n",
    "    v_name: {\"zlib\": True, \"complevel\": 4, \"dtype\": \"float32\", \"chunksizes\": (90, 120, 120)},\n",
    "}\n",
    "\n",
    "dask.config.set(**{\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "# ------------- helpers ------------------\n",
    "def year_from_path(p: str):\n",
    "    m = re.search(r\"(19|20)\\d{4}|(19|20)\\d{2}\", p)  # tries YYYYM? or YYYY\n",
    "    # Prefer pure 4-digit year; fallback if filenames have YYYYMM\n",
    "    m4 = re.search(r\"(19|20)\\d{2}\", p)\n",
    "    return int(m4.group(0)) if m4 else None\n",
    "\n",
    "def open_year(paths):\n",
    "    # Keep each file as one contiguous chunk along time;\n",
    "    # netcdf4 engine + lock=False avoids over-locking\n",
    "    return xr.open_mfdataset(\n",
    "        paths,\n",
    "        combine=\"by_coords\",\n",
    "        engine=\"netcdf4\",\n",
    "        chunks={tname: -1},\n",
    "        parallel=False,\n",
    "        lock=False,\n",
    "        decode_times=True,\n",
    "        drop_variables=None,\n",
    "    )[[u_name, v_name]]\n",
    "\n",
    "# ------------- collect by year ----------\n",
    "paths = sorted(glob(SRC_GLOB))\n",
    "by_year = {}\n",
    "for p in paths:\n",
    "    y = year_from_path(p)\n",
    "    if y is not None:\n",
    "        by_year.setdefault(y, []).append(p)\n",
    "\n",
    "years = [y for y in sorted(by_year) if 1982 <= y <= 2024]\n",
    "print(f\"Found years: {years[0]}–{years[-1]} ({len(years)} years)\")\n",
    "\n",
    "# ------------- convert per year ----------\n",
    "for y in years:\n",
    "    out_nc = OUT_DIR / f\"era5_wind_daily_{y}.nc\"\n",
    "    if out_nc.exists():\n",
    "        print(f\"[SKIP] {out_nc.name} already exists.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{y}] opening {len(by_year[y])} file(s)…\")\n",
    "    ds3h = open_year(by_year[y])\n",
    "\n",
    "    # daily mean for u10 and v10\n",
    "    daily = (\n",
    "        ds3h\n",
    "        .resample({tname: \"1D\"})\n",
    "        .mean()\n",
    "        .astype(\"float32\")        # compact on disk\n",
    "        .sortby(tname)\n",
    "    )\n",
    "\n",
    "    # ensure exactly that year range (handles any file overlap)\n",
    "    daily = daily.sel({tname: slice(f\"{y}-01-01\", f\"{y}-12-31\")})\n",
    "\n",
    "    # write NetCDF\n",
    "    print(f\"[{y}] writing {out_nc.name} …\")\n",
    "    daily.to_netcdf(out_nc, format=\"NETCDF4\", engine=\"netcdf4\", encoding=ENC)\n",
    "\n",
    "    # small sanity print\n",
    "    tt = pd.to_datetime(daily[tname].values)\n",
    "    print(f\"[{y}] saved {tt.size} daily steps; {tt[0].date()} → {tt[-1].date()}\")\n",
    "\n",
    "print(\"✓ Done. Daily files are in:\", OUT_DIR)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, xarray as xr\n",
    "from glob import glob\n",
    "\n",
    "FILES_GLOB = \"/home/Desktop/ERA5 wind data/ERA5_daily_wind_data_30S_30N_20E_110E/era5_wind_daily_*.nc\"\n",
    "\n",
    "# open files path \n",
    "ds = xr.open_mfdataset(sorted(glob(FILES_GLOB)),combine=\"by_coords\",engine=\"netcdf4\",chunks={\"valid_time\": 1752})  \n",
    "\n",
    "# Names\n",
    "tname = \"time\" if \"time\" in ds.dims else \"valid_time\"\n",
    "latn  = \"lat\" if \"lat\" in ds.coords else \"latitude\"\n",
    "lonn  = \"lon\" if \"lon\" in ds.coords else \"longitude\"\n",
    "u_name = \"u10\"; v_name = \"v10\"\n",
    "print(f\"Detected wind variables: u='{u_name}', v='{v_name}' (10 m)\")\n",
    "\n",
    "# coordinates & coverage\n",
    "print(\"\\n1st three cordinated \")\n",
    "def _preview(arr):\n",
    "    vals = arr.values\n",
    "    if vals.ndim == 0:\n",
    "        return f\"{vals!r} (scalar)\"\n",
    "    return f\"{vals[:3]} ... {vals[-3:]}\"\n",
    "for c in [tname, latn, lonn]:\n",
    "    print(f\"{c:>10s}: {_preview(ds[c])}\")\n",
    "if \"expver\" in ds.coords:\n",
    "    print(f\"{'expver':>10s}: {_preview(ds['expver'])}\")\n",
    "\n",
    "# print dimentions\n",
    "print(\"\\nDimensions:\")\n",
    "for k, v in ds.sizes.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# set the variable attributes & chunking data\n",
    "def show_var(name):\n",
    "    da = ds[name]\n",
    "    print(f\"\\nVariable '{name}' attrs\")\n",
    "    for k, v in da.attrs.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    if hasattr(da.data, \"chunks\") and da.data.chunks:\n",
    "        print(\"chunks:\", da.data.chunks)\n",
    "\n",
    "show_var(u_name)\n",
    "show_var(v_name)\n",
    "\n",
    "# quick check of time coverage & values \n",
    "t = pd.to_datetime(ds[tname].values)\n",
    "t0, t1 = t[0], t[-1]\n",
    "print(f\"\\n=== Time Coverage ===\\nStart: {t0}  End: {t1}  Length: {t.size} steps\")\n",
    "\n",
    "# compute quick spatial means for the first and last day to check values\n",
    "u, v = ds[u_name], ds[v_name]\n",
    "first_day = slice(str(pd.to_datetime(t0).date()), str(pd.to_datetime(t0).date()))\n",
    "last_day  = slice(str(pd.to_datetime(t1).date()), str(pd.to_datetime(t1).date()))\n",
    "\n",
    "u_first = u.sel({tname: first_day}).mean([tname, latn, lonn]).compute().item()\n",
    "v_first = v.sel({tname: first_day}).mean([tname, latn, lonn]).compute().item()\n",
    "u_last  = u.sel({tname: last_day}).mean([tname, latn, lonn]).compute().item()\n",
    "v_last  = v.sel({tname: last_day}).mean([tname, latn, lonn]).compute().item()\n",
    "print(f\"\\nsample spatial velocities plotting u10 and v10 in m/s\\n\"\n",
    "      f\"{str(t0.date())}: u={u_first:.2f}, v={v_first:.2f}\\n\"\n",
    "      f\"{str(t1.date())}: u={u_last:.2f},  v={v_last:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
